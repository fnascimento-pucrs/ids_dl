{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDS-IPS with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to build a proof-of-concept model that mimics an IDS-IPS system by predicting whether a stream of network data is malicious or benign. Common IDS-IPS systems use signature-based detection that flags previously-identified malicious network activities. While there are anomaly-based IDS-IPS systems, they often use DBSCAN (Density-Based Spatial Clustering of Applications with Noise) or Gaussian Mixture Models to identify outliers. In this notebook, I will attempt to build a DNN that identifies anomalies in network activity and perform classification. In addition, I will also build an autoencoder network, PCA, DBSCAN, K-Means Clustering and a Gaussian Mixture Model for benchmark purpsoes.\n",
    "\n",
    "The dataset we're using is from ISCX 2017. \n",
    "\n",
    "> CICIDS2017 dataset contains benign and the most up-to-date common attacks, which resembles the true real-world data (PCAPs). It also includes the results of the network traffic analysis using CICFlowMeter with labeled flows based on the time stamp, source and destination IPs, source and destination ports, protocols and attack (CSV files)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essentials\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ignore useless warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "pd.options.display.max_seq_items = 8000\n",
    "pd.options.display.max_rows = 8000\n",
    "\n",
    "import os\n",
    "pd.set_option(\"display.precision\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv', 'Monday-WorkingHours.pcap_ISCX.csv', 'Friday-WorkingHours-Morning.pcap_ISCX.csv', 'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv', 'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv', 'Tuesday-WorkingHours.pcap_ISCX.csv', 'Wednesday-workingHours.pcap_ISCX.csv', 'Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv']\n"
     ]
    }
   ],
   "source": [
    "# Here, we take a look at how many CSV files we have\n",
    "print(os.listdir('./data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load in the dataset and have a look at the raw data. Since we have multiple csv files, I'll load them all at once and concatenate them into one giant dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(818520, 79)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list = []\n",
    "for filename in os.listdir('./data'):\n",
    "    df = pd.read_csv(os.path.join('./data',filename), index_col=None)\n",
    "    df_list.append(df)\n",
    "df = pd.concat(df_list[:2], axis=0, ignore_index=True)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a quick glance, we have 2830743 entries, 78 features and 1 label for the class of the network data. Let's take a closer look at the data. Here, we can see that there are a lot of pre-calculated mean, max, min, std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 818520 entries, 0 to 818519\n",
      "Data columns (total 79 columns):\n",
      " Destination Port               818520 non-null int64\n",
      " Flow Duration                  818520 non-null int64\n",
      " Total Fwd Packets              818520 non-null int64\n",
      " Total Backward Packets         818520 non-null int64\n",
      "Total Length of Fwd Packets     818520 non-null int64\n",
      " Total Length of Bwd Packets    818520 non-null int64\n",
      " Fwd Packet Length Max          818520 non-null int64\n",
      " Fwd Packet Length Min          818520 non-null int64\n",
      " Fwd Packet Length Mean         818520 non-null float64\n",
      " Fwd Packet Length Std          818520 non-null float64\n",
      "Bwd Packet Length Max           818520 non-null int64\n",
      " Bwd Packet Length Min          818520 non-null int64\n",
      " Bwd Packet Length Mean         818520 non-null float64\n",
      " Bwd Packet Length Std          818520 non-null float64\n",
      "Flow Bytes/s                    818438 non-null object\n",
      " Flow Packets/s                 818520 non-null object\n",
      " Flow IAT Mean                  818520 non-null float64\n",
      " Flow IAT Std                   818520 non-null float64\n",
      " Flow IAT Max                   818520 non-null int64\n",
      " Flow IAT Min                   818520 non-null int64\n",
      "Fwd IAT Total                   818520 non-null int64\n",
      " Fwd IAT Mean                   818520 non-null float64\n",
      " Fwd IAT Std                    818520 non-null float64\n",
      " Fwd IAT Max                    818520 non-null int64\n",
      " Fwd IAT Min                    818520 non-null int64\n",
      "Bwd IAT Total                   818520 non-null int64\n",
      " Bwd IAT Mean                   818520 non-null float64\n",
      " Bwd IAT Std                    818520 non-null float64\n",
      " Bwd IAT Max                    818520 non-null int64\n",
      " Bwd IAT Min                    818520 non-null int64\n",
      "Fwd PSH Flags                   818520 non-null int64\n",
      " Bwd PSH Flags                  818520 non-null int64\n",
      " Fwd URG Flags                  818520 non-null int64\n",
      " Bwd URG Flags                  818520 non-null int64\n",
      " Fwd Header Length              818520 non-null int64\n",
      " Bwd Header Length              818520 non-null int64\n",
      "Fwd Packets/s                   818520 non-null float64\n",
      " Bwd Packets/s                  818520 non-null float64\n",
      " Min Packet Length              818520 non-null int64\n",
      " Max Packet Length              818520 non-null int64\n",
      " Packet Length Mean             818520 non-null float64\n",
      " Packet Length Std              818520 non-null float64\n",
      " Packet Length Variance         818520 non-null float64\n",
      "FIN Flag Count                  818520 non-null int64\n",
      " SYN Flag Count                 818520 non-null int64\n",
      " RST Flag Count                 818520 non-null int64\n",
      " PSH Flag Count                 818520 non-null int64\n",
      " ACK Flag Count                 818520 non-null int64\n",
      " URG Flag Count                 818520 non-null int64\n",
      " CWE Flag Count                 818520 non-null int64\n",
      " ECE Flag Count                 818520 non-null int64\n",
      " Down/Up Ratio                  818520 non-null int64\n",
      " Average Packet Size            818520 non-null float64\n",
      " Avg Fwd Segment Size           818520 non-null float64\n",
      " Avg Bwd Segment Size           818520 non-null float64\n",
      " Fwd Header Length.1            818520 non-null int64\n",
      "Fwd Avg Bytes/Bulk              818520 non-null int64\n",
      " Fwd Avg Packets/Bulk           818520 non-null int64\n",
      " Fwd Avg Bulk Rate              818520 non-null int64\n",
      " Bwd Avg Bytes/Bulk             818520 non-null int64\n",
      " Bwd Avg Packets/Bulk           818520 non-null int64\n",
      "Bwd Avg Bulk Rate               818520 non-null int64\n",
      "Subflow Fwd Packets             818520 non-null int64\n",
      " Subflow Fwd Bytes              818520 non-null int64\n",
      " Subflow Bwd Packets            818520 non-null int64\n",
      " Subflow Bwd Bytes              818520 non-null int64\n",
      "Init_Win_bytes_forward          818520 non-null int64\n",
      " Init_Win_bytes_backward        818520 non-null int64\n",
      " act_data_pkt_fwd               818520 non-null int64\n",
      " min_seg_size_forward           818520 non-null int64\n",
      "Active Mean                     818520 non-null float64\n",
      " Active Std                     818520 non-null float64\n",
      " Active Max                     818520 non-null int64\n",
      " Active Min                     818520 non-null int64\n",
      "Idle Mean                       818520 non-null float64\n",
      " Idle Std                       818520 non-null float64\n",
      " Idle Max                       818520 non-null int64\n",
      " Idle Min                       818520 non-null int64\n",
      " Label                          818520 non-null object\n",
      "dtypes: float64(22), int64(54), object(3)\n",
      "memory usage: 493.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go any further, let's rename the columns so that we can access our column data easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['destination_port', 'flow_duration', 'total_fwd_packets',\n",
       "       'total_backward_packets', 'total_length_of_fwd_packets',\n",
       "       'total_length_of_bwd_packets', 'fwd_packet_length_max',\n",
       "       'fwd_packet_length_min', 'fwd_packet_length_mean',\n",
       "       'fwd_packet_length_std', 'bwd_packet_length_max',\n",
       "       'bwd_packet_length_min', 'bwd_packet_length_mean',\n",
       "       'bwd_packet_length_std', 'flow_bytes/s', 'flow_packets/s',\n",
       "       'flow_iat_mean', 'flow_iat_std', 'flow_iat_max', 'flow_iat_min',\n",
       "       'fwd_iat_total', 'fwd_iat_mean', 'fwd_iat_std', 'fwd_iat_max',\n",
       "       'fwd_iat_min', 'bwd_iat_total', 'bwd_iat_mean', 'bwd_iat_std',\n",
       "       'bwd_iat_max', 'bwd_iat_min', 'fwd_psh_flags', 'bwd_psh_flags',\n",
       "       'fwd_urg_flags', 'bwd_urg_flags', 'fwd_header_length',\n",
       "       'bwd_header_length', 'fwd_packets/s', 'bwd_packets/s',\n",
       "       'min_packet_length', 'max_packet_length', 'packet_length_mean',\n",
       "       'packet_length_std', 'packet_length_variance', 'fin_flag_count',\n",
       "       'syn_flag_count', 'rst_flag_count', 'psh_flag_count', 'ack_flag_count',\n",
       "       'urg_flag_count', 'cwe_flag_count', 'ece_flag_count', 'down/up_ratio',\n",
       "       'average_packet_size', 'avg_fwd_segment_size', 'avg_bwd_segment_size',\n",
       "       'fwd_header_length.1', 'fwd_avg_bytes/bulk', 'fwd_avg_packets/bulk',\n",
       "       'fwd_avg_bulk_rate', 'bwd_avg_bytes/bulk', 'bwd_avg_packets/bulk',\n",
       "       'bwd_avg_bulk_rate', 'subflow_fwd_packets', 'subflow_fwd_bytes',\n",
       "       'subflow_bwd_packets', 'subflow_bwd_bytes', 'init_win_bytes_forward',\n",
       "       'init_win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward',\n",
       "       'active_mean', 'active_std', 'active_max', 'active_min', 'idle_mean',\n",
       "       'idle_std', 'idle_max', 'idle_min', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns=lambda x: x.lower().lstrip()\n",
    "          .rstrip().replace(\" \", \"_\"), inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that `Flow Bytes/s`, `Flow Packets/s` and `Labels` are all objects, let's convert the types first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['flow_bytes/s'] = df['flow_bytes/s'].astype('float64')\n",
    "df['flow_packets/s'] = df['flow_packets/s'].astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the data is highly imbalanced. This is actually a huge problem in IDS/IPS models since these imbalanced datasets give rise to a high number of false positives and false negatives. My reasoning would be optimizing the model without overfitting, and allowing more false positives than false negatives.\n",
    "\n",
    "Better safe than sorry..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BENIGN          818484\n",
       "Infiltration        36\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's visualize some of the features to see if there are any interesting distribution / trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_dtypes = ['int64', 'float64']\n",
    "numeric = []\n",
    "\n",
    "# We just want the total instead of the pre-calculated features\n",
    "for i in df.columns:\n",
    "    if df[i].dtype in numeric_dtypes:\n",
    "        if 'mean' in i or 'min' in i or 'max' in i or 'std' in i or 'variance' in i:\n",
    "            pass\n",
    "        else:\n",
    "            numeric.append(i)\n",
    "\n",
    "# visualising data\n",
    "f, ax = plt.subplots(ncols=2, nrows=0, figsize=(12,12))\n",
    "plt.subplots_adjust(right=2)\n",
    "plt.subplots_adjust(top=2)\n",
    "sns.color_palette(\"husl\", 8)\n",
    "\n",
    "for i, feature in enumerate(list(df[numeric]), 1):\n",
    "    if i == 10:\n",
    "        break\n",
    "    plt.subplot(len(list(numeric)), 3, i)\n",
    "    sns.scatterplot(x=feature, y='label', hue='label', palette='Blues', data=df)\n",
    "    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n",
    "    plt.ylabel('Label', size=15, labelpad=12.5)\n",
    "    for j in range(2):\n",
    "        plt.tick_params(axis='x', labelsize=12)\n",
    "        plt.tick_params(axis='y', labelsize=12)\n",
    "    \n",
    "    plt.legend(loc='best', prop={'size': 10})\n",
    "        \n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, a quick look at the correlation between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = df.corr()\n",
    "plt.subplots(figsize=(15,12))\n",
    "sns.heatmap(corr, vmax=0.9, cmap=\"Blues\", square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values\n",
    "def percent_missing(df_cols):\n",
    "    dict_x = {}\n",
    "    for i in range(0,len(df_columns)):\n",
    "        dict_x.update({df_cols[i]: round(data[df_cols[i]].isnull().mean()*100,2)})\n",
    "    return dict_x\n",
    "\n",
    "missing = percent_missing(df.drop(['label'], axis=1).columns)\n",
    "df_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\n",
    "print('Percent of missing data')\n",
    "df_miss[0:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "6867d4e3-be4b-4c07-afaf-e5bf8f1a7f8b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
